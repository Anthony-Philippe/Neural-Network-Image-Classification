{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **PROJECT PYTHON :** Whith 66% accuracy\n"
      ],
      "metadata": {
        "id": "nwpPZHxX4-j5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries :**"
      ],
      "metadata": {
        "id": "NCebEPY05JHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Keras → Permet la construction de réseaux de neurones et l'entrainement de\n",
        "modèle sur des données\n",
        "  * Scikit-learn → Permet l'utilisation d'outils pour la classification, la régression, le clustering, etc\n",
        "  * Matplotlib → Permet de tracer et visualiser les données sous forme de graphiques\n",
        "  "
      ],
      "metadata": {
        "id": "0xkM4OGsmyJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar100\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten, Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vsHLzNEz5Tuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Preprocess the Data**\n"
      ],
      "metadata": {
        "id": "lPiMh_rh5c4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **x_train** ← données des images utilisées pour l'entraînement\n",
        "* **y_train** ← classes correspondant aux images d'entraînement\n",
        "\n",
        "\n",
        "* **x_val** ← données des images utilisées pour la validation\n",
        "* **y_val** ← classes correspondant aux images de validation\n",
        "\n",
        "**Normalisation**\n",
        "<br>*Permet de faciliter l'apprentissage, réduire les pertes et améliorer les performances du modèle.*\n",
        "* **astype('float32')** → convertie les données des images *(pixels)* en valeurs flottente *(float32)*\n",
        "* **/255** → ramène les valeurs RGB des images dans l'interval [0,1] *(pour une meilleure efficacité du model)*\n",
        "\n",
        "**Transformation**\n",
        "<br>*Permet de faciliter la classification et l'utilisation dans un modèle d'apprentissage automatique.*\n",
        "* **to_categorical()** → transforme les classes sous forme de vecteur binaire (0 ou 1)"
      ],
      "metadata": {
        "id": "4btAxiL6m-MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "num_classes = 100\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "XzXVIvks5Wgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data augmentation**"
      ],
      "metadata": {
        "id": "CsR976Uw5hT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Permet de générer de nouvelles données en appliquant des transformations aléatoires aux images provenant du dataset.*\n",
        "* rotation_range → rotations aléatoires jusqu'à 20 degrés\n",
        "* horizontal_flip → retournement horizontal aléatoire des images\n"
      ],
      "metadata": {
        "id": "KowtDvlEnDyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "zWtN6kvJ5k_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the Training data**"
      ],
      "metadata": {
        "id": "JOzyC5Gu5n_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "T1JnDhYA5u9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN architecture**"
      ],
      "metadata": {
        "id": "MYDxO3_75wGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Block 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Block 2\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Block 3\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten and fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='softmax'))"
      ],
      "metadata": {
        "id": "_l3tAhbf53l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile the model**"
      ],
      "metadata": {
        "id": "NELZbTzrTRBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the model with a learning rate schedule\n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True\n",
        ")\n",
        "opt = RMSprop(learning_rate=lr_schedule)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# Implement Early Stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "QfHJEDbcTS9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6tbzIfRX54oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "batch_size = 64  # Define your batch size\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "\n",
        "class SaveBestAccuracyModel(Callback):\n",
        "    def __init__(self, filepath, monitor='val_accuracy', mode='max', verbose=1):\n",
        "        super(SaveBestAccuracyModel, self).__init__()\n",
        "        self.filepath = filepath\n",
        "        self.monitor = monitor\n",
        "        self.mode = mode\n",
        "        self.verbose = verbose\n",
        "        self.best_accuracy = -1\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_accuracy = logs.get(self.monitor)\n",
        "\n",
        "        if current_accuracy is None:\n",
        "            warnings.warn(\"Accuracy metric not found, skipping saving model.\")\n",
        "            return\n",
        "\n",
        "        if (self.mode == 'max' and current_accuracy > self.best_accuracy) or \\\n",
        "           (self.mode == 'min' and current_accuracy < self.best_accuracy):\n",
        "            if self.verbose > 0:\n",
        "                print(f\"\\nEpoch {epoch + 1}: {self.monitor} improved from {self.best_accuracy} to {current_accuracy}. Saving model.\")\n",
        "            self.best_accuracy = current_accuracy\n",
        "            self.model.save(self.filepath, overwrite=True)\n",
        "        else:\n",
        "            if self.verbose > 0:\n",
        "                print(f\"\\nEpoch {epoch + 1}: {self.monitor} did not improve from {self.best_accuracy}. Model not saved.\")\n",
        "\n",
        "# Specify the path where you want to save the best model\n",
        "best_model_path = 'best_model.h5'\n",
        "\n",
        "# Create an instance of the custom callback\n",
        "save_best_accuracy_model = SaveBestAccuracyModel(filepath=best_model_path)\n",
        "\n",
        "# Include the callback in the list of callbacks during model training\n",
        "callbacks_list = [save_best_accuracy_model]\n",
        "# Train the model with callbacks\n",
        "history = model.fit(\n",
        "    datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "# Load the best model for evaluation\n",
        "best_model = keras.models.load_model(best_model_path)\n"
      ],
      "metadata": {
        "id": "InQtBKPk58UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model**"
      ],
      "metadata": {
        "id": "YN4R6AlX5-2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On évalue les performances du modèle sur l'ensemble de données de validation (images et classes)"
      ],
      "metadata": {
        "id": "eErgc-oWnL1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Best Model - Test loss:', test_loss)\n",
        "print('Best Model - Test accuracy:', test_accuracy * 100)"
      ],
      "metadata": {
        "id": "fd2J-uJF6CZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display the result**"
      ],
      "metadata": {
        "id": "BU4qNfd96Imo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On affiche sous forme de 2 graphiques les résultats (pertes et précision en fonction du nombre d'épochs) obtenu pendant l'entrainement et la validation du modèle"
      ],
      "metadata": {
        "id": "XytvZOaRnTx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGeURtOE3Qkr"
      },
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], linestyle='dashed', label='val')\n",
        "plt.title('Loss vs. Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(loc='center right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], color='green', label='train')\n",
        "plt.plot(history.history['val_accuracy'], linestyle='dashed', color='red', label='val')\n",
        "plt.title('Accuracy vs. Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(loc='center right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}